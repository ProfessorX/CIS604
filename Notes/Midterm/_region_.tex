\message{ !name(Review-Notes.tex)}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
\documentclass[twocolumn]{article}

% Packages
% \usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=10mm]{geometry}
\usepackage{xcolor}
\usepackage{graphicx}



% Housekeeping
\pagestyle{empty}


\begin{document}

\message{ !name(Review-Notes.tex) !offset(-3) }

\section{Artificial Intelligence}
\label{sec:artif-intell}

%%% Ideas:
% There should be more than these note points. Add some when you do
% the second time reviewing.
\begin{itemize}
% \item Life is fucking awesome in the United Arab Emirates!
% \item Life is a bitch. Fuck it or leave it, choose one.
\item An \textbf{agent} is an entity that can perceive and act. This
  course is about designing rational agents. 
\item Rational behavior: doing the right thing. 
\item Environment Types: Fully observable; Deterministic; Episodic;
  Static, Discrete; Single-agent. The counter part: partially
  observable; stochastic; sequential; dynamic; continuous;
  multi-agent. 
\item An agent is anything that can be viewed as perceiving its
  environment through sensors and acting upon that environment through
  actuators.
\item Being rational means maximizing your \textbf{expected
    utility}. And a better title for this course would be
  \textbf{Computational Rationality}.
\item \textbf{Rational:} maximally achieving pre-defined goals.
\item \textbf{Rationality:} only concerns what decisions are made (not
  the thought process behind them)
\item \textbf{Utility:} Goals are expressed in the terms of the
  utility of outcomes. And CS188 thinks that being rational means
  maximizing your expected utility.
\item \textbf{Automation:} Applied AI involves many kinds of
  automation. Scheduling, route planning, medical diagnosis, web
  search engines, spam classifiers, automated help desks, fraud
  detection, product recommendations. ``Did any one of these remind
  you of subtopics and projects in Machine Learning?'' 
\item \textbf{Agent:} An agent is an entity that perceives and acts. A
  rational agent selects actions that maximize its \emph{expected}
  utility. 
\item Making decision, reasoning under Uncertainty, and their
  applications. 
\end{itemize}


\section{Problem Solving}
\label{sec:problem-solving}

\begin{itemize}
% \item Life is fucking awesome in the United Arab Emirates!
\item A search problem consists of 
  \begin{itemize}
  \item a state space
  \item a successor function (namely \textbf{update function} in data
    mining algorithm series, more namely \textbf{recursion} in
    bullshit technology.)
  \item a start state (\textbf{initial value}), goal test
    (\textbf{terminating value}) and path cost function (\textbf{we
      say weights in Graph Theory})
  \item Does any one of the above reminds you of \textbf{recursion}?
  \end{itemize}
\item Problems are often modelled as a state space, a set of states
  that a problem can be in. The set of states forms a graph where two
  states are \textbf{connected} if there is an operation that can be
  performed to transform the first state into the second.
\item A solution is a sequence of actions (a plan) which transforms
  the start state to a goal state.
\item \textbf{State space graph}: A mathematical representation of a
  search problem.
\item \textbf{Search Trees} 
  \begin{itemize}
  \item This is a ``what-if'' tree of plans and outcomes
  \item For most problems, we can never actually build the whole tree
  \end{itemize}
\item \textbf{General Tree Search} Frontier; Expansion; Exploration
  Strategy.
\item \textbf{States vs. Nodes} Nodes in state space graphs are
  problem states; Nodes in search trees are plans. {\color{red}The same problem
  state may be achieved by multiple search tree nodes.}
\item \textbf{Graph Search} Graph Search still  produces a search
  tree; Graph search is almost always better than tree search.
\item DFS graph search needs to store ``explored set'', which is
  $O(b^{m})$. However, \textbf{DFS is optimal} when the search tree is finite,
  all action costs are identical and all solutions have the same
  length. However limitating this may sound, there is an important
  class of problems that satisfies these conditions: the CSPs
  (constraint satisfaction problems). Maybe all the examples you
  thought about fall in this (rather common) category. 
\item The breadth first search and iterative deepening are
  conceptually and computationally the same. The only difference is
  the ``space'' (we call them \textbf{memory}) would be partially
  saved by iterative deepening search.
\item \textbf{Heuristics} estimate of how close a node is to a goal;
  Designed for a particular search problem. 
% \item Life is fucking awesome in the United Arab Emirates.
\item \textbf{A star search} Uniform-cost orders by \textbf{path cost}, or
  backward cost  $g(n)$; Best-first orders by \textbf{distance} to goal, or
  forward cost $h(n)$. A* Search orders by the sum:
  $f(n)=g(n)+h(n)$. The distance is an estimated one.
\item When A* terminates its search, it has, by definition, found a
  path whose actual cost is lower than the estimated cost of any path
  through any node on the frontier. But since those estimates are
  optimistic, A* can safely ignore those nodes.
\item In general, most natural admissible heuristics tend to be
  consistent, especially if from relaxed problems. 
% \item Something extra while reviewing online courses.
\item Types of agents: reflex agents, planning agents (optimal
  vs. complete planning).
\end{itemize}

\subsection{Uninformed Search}
\label{sec:uninformed-search}

\begin{itemize}
\item \textbf{State Space:} A state space is also an abstraction of
  the world. A successor function \textbf{models} how your world
  works (namely, evolves and response to your actions). 
\item \textbf{Search Problems:} They are just models, aka, no more
  than models in the mathematical sense.
\item \textbf{World State:} Includes every last detail of the
  environment. 
\item \textbf{Search State:} Keeps only the details needed for
  planning (namely, abstraction). Because only with abstraction can we
  solve problems smoothly.
\item \textbf{Search Trees:} For most problems, we can never actually
  build the whole tree. (So we \textbf{ignore} most of the tree.)
\item \textbf{Complete:} Guaranteed to find a solution if one exists?
  \textbf{Optimal:} Guaranteed to find the least cost path?
\item \textbf{DFS vs BFS:} When will one outperform the other?
\item \textbf{Uniform Cost Search:} Expand a cheapest node first. Thus
  fringe is a \textbf{priority queue}. (priority, cumulative cost,
  namely, add them up!) Therefore it's complete and optimal! But it
  explores options in every ``direction''. And this algorithm shows
  \textbf{no information} about goal location.
\item Search operates over \textbf{models} (namely, abstractions) of
  the world. Planning is all ``in simulation'', therefore your search
  is only as good as your model is.
\end{itemize}

\subsection{Informed Search}
\label{sec:informed-search}

\begin{itemize}
\item \textbf{Informed Search:} Inject information of where the goal
  might be. Key idea: Heuristics.
\item \textbf{Successor Function:} If I do this, what will
  happen, in my model.
\item \textbf{Search Heuristics:} Something tells you that whether you
  are getting close to the goal, or not. It's a function that
  \emph{estimates} how close a state is to a goal. It's designed for a
  particular search. Examples: Manhattan distance, euclidean
  distance. (They are not perfect, but they are \emph{at least}
  something.) 
\item \textbf{Greedy Search:} A common case: best-first takes you
  straight to the (wrong) goal.
\item \textbf{A* Search:} Revised. Combine both UCS and Greedy,
  namely, tortoise and rabbit. Uniform-cost orders by path cost, or
  backward cost. Greedy orders by goal proximity, or forward cost.
\item \textbf{A* Search:} Stop when you \textbf{dequeue} a goal from
  the fringe. Lesson: We need estimates to be less than actual costs.
\item \textbf{Admissibility:} Admissible (optimistic) heuristics slow
  down bad plans but never out-weight true costs. Inadmissible is just
  a fancy name for, \textbf{pessimistic}, it traps good plans on the
  fringe. 
\item A heuristic \textbf{h} is \textbf{admissible} (optimistic) if: 
  \begin{equation}
    \label{eq:1}
    0\leq h(n)\leq h^{*}(n)
  \end{equation}
  where $h^{*}(n)$ is a true cost to a nearest goal. Thus coming up
  with admissible heuristics is most of what's involved in using
  $A^{*}$ in practice. $A^{*}$ is not problem specific, but your
  heuristic is.
\item \textbf{Crating Admissible Heuristics:} Most of the work in
  solving hard search problems optimally is in coming up with
  admissible heuristics. Often, admissible heuristics are solutions to
  \textbf{relaxed problems}, where new actions are available.
\item \textbf{Graph Search:} For tree search, if it fails to detect
  repeated states can cause exponentially more work. Idea:
  \textbf{never expand} a state twice.
\item Important: (in python's idea) store the closed set as a set, not
  a list. In Lisp's concept, make it a hash table (it is verified,
  just use hash table in Lisp).
\item \textbf{Consistency of Heuristics:} real cost should be larger
  or equal than cost implied by heuristic. (Namely, please be
  \textbf{Conservative}, aka guess ``smally'' rather than
  ``biggerly''.) Implication: $f$ value along a path never decreases. 
\item \textbf{Optimality:} For tree search, requires heuristic
  admissible; for graph search, requires consistent. And consistency
  implies admissibility.
\item \textbf{Heuristics:} The design of this number (function) is
  key, often use \textbf{relaxed problems}.
\end{itemize}

\subsection{Constraint Satisfaction Problems}
\label{sec:constr-satisf-probl}

\begin{itemize}
% \item Life is fucking awesome in UAE!
\item \textbf{Search:} a single agent, deterministic actions, fully
  observed state, discrete state space.
\item \textbf{Planning:} a sequence of actions. The \textbf{path} to
  the goal is the important thing.
\item \textbf{Identification:} assignments to variables. The goal
  itself is important, not the path.
\item \textbf{CSP:} a special subset of search problems. State is
  defined by \textbf{variable $X_{i}$} with values from a domain $D$
  (sometimes $D$ depends on $i$). Goal test is a set of constraints
  specifying allowable combinations of values for subsets of
  variables. 
\item CSP allows useful general-purpose algorithms with more power
  than standard search algorithms. (Namely, add more ``rules'', walk
  through (traverse) less paths.)
\item \textbf{CSP Varieties:} Discrete variables; continuous
  variables.
\item \textbf{Varieties of Constraints:} Unary; Binary; Higher-order
  constraints. Or Preferences (soft constraints).
\item \textbf{Backtrack Search:} The basic uninformed algorithm for
  solving CSPs. Namely, recursion. One variable at a time; check
  constraints as you go (Online shit? Incremental goal test). So
  backtracking is equal to DFS add variable ordering and add
  fail-on-violation. 
\item \textbf{Improve Backtracking:} Ordering; Filtering; Structure. 
\item \textbf{Filtering:} Keep track of domains for unassigned
  variables and cross off bad options. Namely, build a mathematical
  filter. Namely, ask (cond, else) when doing forward checking.
\item \textbf{Forward Checking:} Enforcing consistency of arcs
  pointing to each new assignment. 
\item \textbf{Arc Consistency:} It still runs inside a backtrack
  search.
\item \textbf{Ordering:} Minimum Remaining Values. Variable ordering,
  \textbf{always} choose the variable with the \textbf{fewest} legal
  left values in its domain, given a choice of variables.
\item What the hell is CSP? Variables; Domains;
  Constraints---Implicit, Explicit, Unary/Binary/N-ary. Goals: find
  any solution; find all; find best, etc.
\item \textbf{K-Consistency:} For each k nodes, any consistent
  assignment to $k-1$ can be extended to the $k^{th}$ node.
\item Suppose a graph of $n$ variables can be broken into subproblems
  of only $c$ variables. Example: $n=80, d=2, c=20$. But this ``crap''
  is somehow impractical. 
\item \textbf{Tree-Structured CSPS:} Theorem, if the constraint graph
  has no loops, the CSP can be solved in $O(nd^{2})$ time. For general
  CSPs, worst case is $O(d^{n})$. This also applies to probabilistic
  reasoning: an example of the relation between syntactic restrictions
  and the complexity of reasoning.
\item \textbf{Nearly Tree-Structured CSPs:} Cutset conditioning:
  instantiate (in all ways) a set of variables such that the remaining
  constraint graph is a \textbf{tree}.
\item \emph{Sorry this is in ai class, everything is hard.---CS188}
\item \textbf{Tree Decomposition:} Create a tree-structured graph of
  mega-variables. Each mega-variables encodes part of the original
  CSP. 
\item CSPs are a special kind of search problems where states are
  partial assignments and goal test is defined by constraints. The
  basic solution is backtrack search.
\item \textbf{Local Search:} (yet another fancy name of EM algorithm.)
  It improves a single option until you can't make it
  better. (\textbf{No fringe!}) 
\item Generally local search is much faster and more memory
  efficient. But it is also \textbf{incomplete and suboptimal}.
\item \textbf{Hill Climbing:} Simple general idea---Start wherever,
  repeat: move to the best neighboring state; if no neighbors better
  than current, quit.
\item \textbf{Simulated Annealing:} Idea, escape local maxima by
  allowing downhill moves.
\item The more downhill steps you need to escape a local optimum, the
  less likely you are to ever make them all in a row. Therefore people
  think hard about ridge operators which let you jump around the space
  in better ways.
\item \textbf{Genetic Algorithms:} It uses a natural selection
  metaphor---keep best N hypotheses at each step based on a fitness
  function; Also have pairwise crossover operators, with optional
  mutation to give variety. 
\end{itemize}

\subsection{Adversarial Search}
\label{sec:adversarial-search}

\begin{itemize}
% \item Life is fucking awesome in the United Arab Emirates!
\item \textbf{Meaning:} How to decide how to act, when there is an
  adversary in ``your world (model, abstraction, etc.)''.
\item Monte Carlo methods are just a fancy name for
  \textbf{randomized} methods.
\item \textbf{Pacman:} Behavior from \textbf{Computation}.
\item \textbf{Axes:} Deterministic or stochastic? One, two or more
  players? Zero sum? Perfect information (can you see the state)?
\item For this course, we want algorithms for calculating a
  \textbf{strategy (policy)} which recommends a \textbf{move} from
  each state.
\item Different from search: we do not \textbf{control} our
  opponent. We need to give out \textbf{policies}.
\item One possible formalization is: States, Players, Actions,
  Transition Function $S\times A\rightarrow S$, Terminal Test $S\rightarrow
  \{t,f\}$, Terminal Utilities $S\times P\rightarrow R$.
\item Players usually take turns; Actions may depend on player/state;
  Terminal utilities tells us how much it's worth to \textbf{each of
    the players}. 
\item \textbf{Zero-Sum Games:} Let us think of a single value that one
  maximizes and the other minimizes.
\item \textbf{General Games:} Agents have independent
  utilities. Cooperation, indifference, competition, and more are all
  possible. 
\item  \textbf{Value} of a state: The \textbf{best} achievable outcome
  (utility) from that state.
\item \textbf{Minimax Values:} States Under Opponent's Control:
  $V(s')= min~V(s)$ States Under Agent's Control: \textbf{Maximize}
  out of all possible ``worst'' results your 
\end{itemize}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\message{ !name(Review-Notes.tex) !offset(-340) }
